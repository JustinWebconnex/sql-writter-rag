{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confluence section\n",
    "!pip install python-dotenv requests langchain-community\n",
    "!pip install langchain\n",
    "!pip install atlassian-python-api\n",
    "!pip install lxml\n",
    "!pip install tiktoken\n",
    "!pip install pandas\n",
    "!pip install boto3\n",
    "!pip install markdownify\n",
    "\n",
    "# Github section\n",
    "\n",
    "!pip install os-sys\n",
    "!pip install GitPython\n",
    "!pip install langchain\n",
    "!pip install langchain_community\n",
    "!pip install PyGithub\n",
    "!pip install base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting all confluence data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.document_loaders import ConfluenceLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "# Attempt to import BeautifulSoup and lxml, handle the exception if lxml is not installed\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Attempt to import BeautifulSoup and lxml, handle the exception if lxml is not installed\") from e\n",
    "\n",
    "\n",
    "# Environment variable configuration\n",
    "sys.path.append('../')\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Confluence configuration\n",
    "CONFLUENCE_URL = os.getenv(\"CONFLUENCE_URL\")\n",
    "CONFLUENCE_API_KEY = os.getenv(\"CONFLUENCE_API_KEY\")\n",
    "CONFLUENCE_USERNAME = os.getenv(\"CONFLUENCE_USERNAME\")\n",
    "CONFLUENCE_SPACE_KEY = os.getenv(\"CONFLUENCE_SPACE_KEY\")\n",
    "\n",
    "# Load documents from Confluence\n",
    "loader = ConfluenceLoader(\n",
    "    url=CONFLUENCE_URL,\n",
    "    username=CONFLUENCE_USERNAME,\n",
    "    api_key=CONFLUENCE_API_KEY\n",
    ")\n",
    "docs = loader.load(\n",
    "    space_key=CONFLUENCE_SPACE_KEY,\n",
    "    # limit=1,\n",
    "    # max_pages=5,\n",
    "    keep_markdown_format=True\n",
    ")\n",
    "\n",
    "# Split documents based on Markdown headers\n",
    "def split_markdown_documents(docs):\n",
    "    # Markdown\n",
    "    headers_to_split_on = [\n",
    "            (\"#\", \"Title 1\"),\n",
    "            (\"##\", \"Subtitle 1\"),\n",
    "            (\"###\", \"Subtitle 2\"),\n",
    "    ]\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "    # Split based on markdown and add original metadata\n",
    "    md_docs = []\n",
    "    for doc in docs:\n",
    "        md_doc = markdown_splitter.split_text(doc.page_content)\n",
    "        for i in range(len(md_doc)):\n",
    "            md_doc[i].metadata = md_doc[i].metadata | doc.metadata\n",
    "        md_docs.extend(md_doc)\n",
    "\n",
    "    # RecursiveTextSplitter\n",
    "    # Chunk size big enough\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=20,\n",
    "        separators=[r\"\\n\\n\", r\"\\n\", r\"(?<=\\. )\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    splitted_docs = splitter.split_documents(md_docs)\n",
    "    return splitted_docs\n",
    "\n",
    "\n",
    "texts = split_markdown_documents(docs)\n",
    "\n",
    "# print first 10 chunks\n",
    "def pretty_print(chunks, limit=10):\n",
    "    for i, chunk in enumerate(chunks[:limit]):\n",
    "        print(f\"Chunk {i+1} Content:\\n{chunk.page_content}\\n---\\nMetadata:\\n{chunk.metadata}\\n{'='*50}\\n\")\n",
    "\n",
    "# pretty_print(texts)\n",
    "\n",
    "def create_dataframe(texts):\n",
    "    # Prepare the data for the DataFrame\n",
    "    data = {\n",
    "        \"source\": [\"confluence\"] * len(texts),\n",
    "        \"page_content\": [text.page_content for text in texts],\n",
    "        \"metadata\": [text.metadata for text in texts]\n",
    "    }\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "directory_path = \"../sources_data\"\n",
    "\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming texts is already populated from the split_markdown_documents function\n",
    "df_confluence = create_dataframe(texts)\n",
    "file_path = os.path.join(directory_path, \"df_confluence.csv\")\n",
    "df_confluence.to_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webpage Ingestion into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import re\n",
    "\n",
    "def is_valid_url(url, base_url):\n",
    "    \"\"\" Check if a URL is valid and belongs to the same domain without being a media file. \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return (parsed.scheme in ['http', 'https'] and\n",
    "            urlparse(base_url).netloc == parsed.netloc and\n",
    "            not parsed.path.endswith(tuple(['.png', '.jpg', '.jpeg', '.svg', '.pdf'])))\n",
    "\n",
    "def crawl_and_extract_text(url, depth=3, delay=1.0):\n",
    "    \"\"\" Crawl a website to a specified depth and extract text, with delays between requests. \"\"\"\n",
    "    visited = set()\n",
    "    pages_to_visit = [(url, 0)]\n",
    "    all_texts = []\n",
    "\n",
    "    while pages_to_visit:\n",
    "        current_page, current_depth = pages_to_visit.pop(0)\n",
    "        if current_page not in visited and current_depth <= depth:\n",
    "            visited.add(current_page)\n",
    "            try:\n",
    "                response = requests.get(current_page)\n",
    "                response.raise_for_status()  # Ensure successful response\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "                all_texts.append({'page_content': text, 'metadata': {'url': current_page}})\n",
    "                if current_depth < depth:\n",
    "                    for link in soup.find_all('a', href=True):\n",
    "                        full_url = urljoin(current_page, link['href'])\n",
    "                        if is_valid_url(full_url, url):\n",
    "                            pages_to_visit.append((full_url, current_depth + 1))\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Failed to retrieve {current_page}: {e}\")\n",
    "            time.sleep(delay)  # Delay between requests\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "class TextSplitter:\n",
    "    def __init__(self, chunk_size=1000):\n",
    "        # Define the chunk size or use a default value\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def split_text(self, text):\n",
    "        # Use regex to split the text into sentences\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', text)\n",
    "        chunks = []\n",
    "\n",
    "        current_chunk = \"\"\n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) <= self.chunk_size:\n",
    "                current_chunk += sentence + \" \"\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "def process_texts(all_texts):\n",
    "    splitter = TextSplitter()\n",
    "    processed_texts = []\n",
    "    for text_data in all_texts:\n",
    "        chunks = splitter.split_text(text_data['page_content'])\n",
    "        for chunk in chunks:\n",
    "            processed_texts.append({'content': chunk, 'url': text_data['metadata']['url']})\n",
    "    return processed_texts\n",
    "\n",
    "#  usage:\n",
    "base_url = 'https://www.webconnex.com/'\n",
    "all_texts = crawl_and_extract_text(base_url, depth=3)\n",
    "web_processed_texts = process_texts(all_texts)\n",
    "\n",
    "def create_web_dataframe(all_texts):\n",
    "    \"\"\" Convert the list of text dictionaries into a DataFrame with the specified structure. \"\"\"\n",
    "    # Adjust the structure to include 'source' and ensure 'metadata' is a dictionary\n",
    "    data = {\n",
    "        'source': ['web'] * len(all_texts),\n",
    "        'page_content': [text['content'] for text in all_texts],\n",
    "        'metadata': [{'url': text['url']} for text in all_texts]\n",
    "    }\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Processed web texts \n",
    "web_df = create_web_dataframe(web_processed_texts)\n",
    "file_path = os.path.join(directory_path, \"df_web.csv\")\n",
    "web_df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github ingestion into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change repo name to the repo that you want to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"webconnex/data-pipeline-sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from git import Repo\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "from github import Github\n",
    "import base64\n",
    "# Get the home directory path\n",
    "home_dir = os.getcwd()\n",
    "# Clone the repository\n",
    "GITHUB_API_KEY = os.getenv(\"GITHUB_API_KEY\")\n",
    "g = Github(GITHUB_API_KEY)\n",
    "repo = g.get_repo(repo_name)\n",
    "\n",
    "\n",
    "# Create a directory for the repository\n",
    "repo_dir = os.path.join(home_dir, *repo_name.split('/'))\n",
    "os.makedirs(repo_dir, exist_ok=True)\n",
    "\n",
    "# Download files from the repository\n",
    "contents = repo.get_contents(\"\")\n",
    "\n",
    "def download_file(file_content):\n",
    "    if file_content.size > 1000000:  # Check if the file is larger than 1MB\n",
    "        print(f\"File {file_content.path} is too large for direct API download.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        if file_content.encoding == 'base64':\n",
    "            file_data = base64.b64decode(file_content.content)\n",
    "            file_path = os.path.join(repo_dir, file_content.path)\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "            with open(file_path, 'wb') as file:\n",
    "                file.write(file_data)\n",
    "            print(f\"Downloaded {file_content.path}\")\n",
    "        else:\n",
    "            print(f\"Skipped {file_content.path} due to unsupported encoding or empty content\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {file_content.path}: {str(e)}\")\n",
    "\n",
    "while contents:\n",
    "    file_content = contents.pop(0)\n",
    "    if file_content.type == \"dir\":\n",
    "        contents.extend(repo.get_contents(file_content.path))\n",
    "    elif file_content.type == \"file\":\n",
    "        download_file(file_content)\n",
    "    elif file_content.type == \"symlink\":\n",
    "        print(f\"Skipped symlink {file_content.path}\")\n",
    "    else:\n",
    "        print(f\"Skipped {file_content.path} due to unsupported file type or content\")\n",
    "\n",
    "# Load supported programming languages using LanguageParser\n",
    "supported_languages = [Language.PYTHON, Language.JS, Language.JAVA, Language.GO, Language.CPP]\n",
    "supported_documents = []\n",
    "\n",
    "for lang in supported_languages:\n",
    "    loader = GenericLoader.from_filesystem(\n",
    "        repo_dir,\n",
    "        glob=\"**/*\",\n",
    "        suffixes=[\".py\", \".js\", \".java\", \".go\", \".cpp\", \".c\", \".cc\", \".cxx\", \".h\", \".hpp\"],\n",
    "        parser=LanguageParser(language=lang, parser_threshold=500),\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    supported_documents.extend(documents)\n",
    "\n",
    "# Load unsupported file types using a generic parsing algorithm\n",
    "unsupported_loader = GenericLoader.from_filesystem(\n",
    "    repo_dir,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".sql\", \".yml\", \".txt\", \".dockerfile\"],\n",
    "    parser=LanguageParser(parser_threshold=500)\n",
    ")\n",
    "unsupported_documents = []\n",
    "try:\n",
    "    unsupported_documents = unsupported_loader.load()\n",
    "except ValueError as e:\n",
    "    print(f\"Error loading documents: {str(e)}\")\n",
    "\n",
    "# Combine all documents\n",
    "documents = supported_documents + unsupported_documents\n",
    "\n",
    "# Split the documents into chunks\n",
    "supported_texts = []\n",
    "for lang in supported_languages:\n",
    "    lang_documents = [doc for doc in supported_documents if doc.metadata['language'] == lang]\n",
    "    splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=lang, chunk_size=2000, chunk_overlap=200\n",
    "    )\n",
    "    lang_texts = splitter.split_documents(lang_documents)\n",
    "    supported_texts.extend(lang_texts)\n",
    "\n",
    "generic_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "unsupported_texts = generic_splitter.split_documents(unsupported_documents)\n",
    "\n",
    "texts = supported_texts + unsupported_texts\n",
    "\n",
    "def create_github_dataframe(texts):\n",
    "    \"\"\" Convert GitHub text chunks into a DataFrame with the specified structure. \"\"\"\n",
    "    # Adjust structure to include 'source' and ensure 'metadata' contains necessary details\n",
    "    data = {\n",
    "        'source': ['github_platform_code'] * len(texts),\n",
    "        'page_content': [text.content for text in texts],\n",
    "        'metadata': [{'file_path': text.metadata['path'], 'language': text.metadata.get('language')} for text in texts]\n",
    "    }\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Process and create DataFrame\n",
    "github_df = create_github_dataframe(texts)\n",
    "file_path = os.path.join(directory_path, \"github_code.csv\")\n",
    "github_df.to_csv(file_path)\n",
    "print(f\"Total number of documents: {len(documents)}\")\n",
    "print(f\"Total number of chunks: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was for the code, now we gonna take care the other repo information like commits, PR and more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def collect_github_data(repo):\n",
    "    github_docs = []\n",
    "\n",
    "    # Collect branches\n",
    "    branches = repo.get_branches()\n",
    "    for branch in branches:\n",
    "        github_docs.append({\n",
    "            'content': f\"Branch: {branch.name}\",\n",
    "            'type': 'branch',\n",
    "            'repository': repo_name\n",
    "        })\n",
    "\n",
    "    # Collect pull requests\n",
    "    pulls = repo.get_pulls(state='all', sort='created', base='main')\n",
    "    for pr in pulls:\n",
    "        github_docs.append({\n",
    "            'content': f\"PR #{pr.number}: {pr.title} by {pr.user.login}\",\n",
    "            'type': 'pull request',\n",
    "            'repository': repo_name\n",
    "        })\n",
    "\n",
    "    # Collect commits\n",
    "    commits = repo.get_commits(sha='main')\n",
    "    for commit in commits:\n",
    "        commit_data = f\"Commit {commit.sha}\\nAuthor: {commit.commit.author.name}\\nDate: {commit.commit.author.date}\\nMessage: {commit.commit.message}\\n\"\n",
    "        files = commit.files\n",
    "        for file in files:\n",
    "            commit_data += f\"{file.filename} +{file.additions} -{file.deletions} changes\\n\"\n",
    "            if file.patch:\n",
    "                commit_data += f\"Patch:\\n{file.patch}\\n\"\n",
    "        github_docs.append({\n",
    "            'content': commit_data,\n",
    "            'type': 'commit',\n",
    "            'repository': repo_name\n",
    "        })\n",
    "\n",
    "    return github_docs\n",
    "\n",
    "def split_github_documents(github_docs):\n",
    "    splitted_docs = []\n",
    "    for doc in github_docs:\n",
    "        # Split each document into chunks of up to 512 characters\n",
    "        content_length = len(doc['content'])\n",
    "        for i in range(0, content_length, 512):\n",
    "            chunk_content = doc['content'][i:i+512]\n",
    "            splitted_docs.append({\n",
    "                'page_content': chunk_content,\n",
    "                'metadata': {\n",
    "                    'type': doc['type'],\n",
    "                    'repository': doc['repository']\n",
    "                }\n",
    "            })\n",
    "    return splitted_docs\n",
    "\n",
    "def save_texts(texts, filename='github_data.pkl'):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(texts, file)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# usage\n",
    "github_docs = collect_github_data(repo)\n",
    "splitted_texts = split_github_documents(github_docs)\n",
    "# save_texts(splitted_texts)\n",
    "\n",
    "def create_github_dataframe(github_docs):\n",
    "    \"\"\" Convert GitHub documents into a DataFrame with standardized structure. \"\"\"\n",
    "    data = {\n",
    "        'source': ['github_platform_info'] * len(github_docs),\n",
    "        'page_content': [doc['content'] for doc in github_docs],\n",
    "        'metadata': [{'type': doc['type'], 'repository': doc['repository']} for doc in github_docs]\n",
    "    }\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Process and create DataFrame\n",
    "github_docs = collect_github_data(repo)\n",
    "github_df = create_github_dataframe(splitted_texts)\n",
    "file_path = os.path.join(directory_path, \"github_info.csv\")\n",
    "github_df.to_csv(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
